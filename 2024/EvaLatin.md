***
<p style="text-align: center;"><b>LT4HALA 2024</b></p>
<p style="text-align: center;">--<a href="index">Home</a>--&nbsp;&nbsp;--<a href="CFP">CFP</a>--&nbsp;&nbsp;--EvaLatin--&nbsp;&nbsp;--<a href="EvaHan">EvaHan</a>--&nbsp;&nbsp;--<a href="Program">Program</a>--&nbsp;&nbsp;--<a href="organization">Organization</a>--</p>
***

## EvaLatin

![](LOGO.png)

- [Introduction](#introduction)
- [Important Dates](#important-dates)
- [Data](#data)
- [How to participate](#how-to-participate)

___

### INTRODUCTION

The LT4HALA 2024 workshop will also be the venue of the third edition of *EvaLatin*, the evaluation campaign totally devoted to the evaluation of NLP tools for Latin. The campaign is designed with the aim of answering two questions:
- How can we promote the development of resources and language technologies for the Latin language?
- How can we foster collaboration among scholars working on Latin and attract researchers from different disciplines?

EvaLatin 2024 edition will have 2 task, i.e. Dependency Parsing and Emotion Polarity Detection. Shared test data and an evaluation script will be provided to the participants who will choose to participate in either one or all tasks. 

EvaLatin 2024 is organized by Rachele Sprugnoli, Federica Iurescia and Marco Passarotti.

### IMPORTANT DATES
- 22 December 2023: guidelines available
- Evaluation Window I - Task: Syntactic Parsing
 - 14 February 2024: test data available
 - 24 February 2024: system results due to organizers
- Evaluation Window II - Task: Emotion Polarity Detection
 - 28 February 2024: test data available
 - 10 March 2024: system results due to organizers
- 25 March 2024: reports due to organizers
- 01 April 2024: short report review deadline
- 14 April 2024: camera ready version of reports due to organizers


### DATA
Dependency parsing will be based on the Universal Dependencies framework. **No** specific training data will be released but participants will be allowed to use the Latin treebanks already available: the main challenge will be to understand which treebank (or combination of treebanks) is the most suitable to deal with new test data. Test data will be both prose and poetic texts from different time periods. Even for the emotion polarity detection task, **no** training data will be released but the organizers will provide an annotation sample and a manually created polarity lexicon. In this task participants will have the opportunity to test unsupervised or cross-language approaches. Test data will be poetic texts from different time periods.

### HOW TO PARTICIPATE
Participants will be required to submit their runs and to provide a technical report that should include a brief description of their approach, focusing on the adopted algorithms, models and resources, a summary of their experiments, and an analysis of the obtained results.Technical reports will be included in the proceedings as short papers: the maximum length is 4 pages (excluding references) and they should follow the [LREC-COLING 2024 official format](https://lrec-coling-2024.org/authors-kit/). Reports will receive a light review (we will check for the correctness of the format, the exactness of results and ranking, and overall exposition). 

Participants are allowed to use any approach (e.g. from traditional machine learning algorithms to Large Language Models) and any resource (annotated and non-annotated data, embeddings): all approaches and resources are expected to be described in the systems' reports.


***
<p style="text-align: center;">Back to the <a href="https://circse.github.io/LT4HALA/"><b>Main Page</b></a></p>
***
